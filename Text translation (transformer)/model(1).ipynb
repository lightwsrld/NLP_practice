{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oS9NHsed-wu"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TranslateTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_size,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        max_len,\n",
        "    ):\n",
        "        super(TranslateTransformer, self).__init__()\n",
        "        self.srcEmbeddings = nn.Embedding(src_vocab_size, embedding_size)\n",
        "        self.trgEmbeddings = nn.Embedding(trg_vocab_size, embedding_size)\n",
        "        self.srcPositionalEmbeddings = nn.Embedding(max_len, embedding_size)\n",
        "        self.trgPositionalEmbeddings = nn.Embedding(max_len, embedding_size)\n",
        "        self.transformer = nn.Transformer(\n",
        "            embedding_size,\n",
        "            num_heads,\n",
        "            num_encoder_layers,\n",
        "            num_decoder_layers,\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
        "\n",
        "        return src_mask.to(device)\n",
        "\n",
        "    def forward(self, x, trg):\n",
        "        src_seq_length = x.shape[0]\n",
        "        N = x.shape[1]\n",
        "        trg_seq_length = trg.shape[0]\n",
        "\n",
        "        src_positions = (\n",
        "            torch.arange(0, src_seq_length)\n",
        "            .reshape(src_seq_length, 1) + torch.zeros(src_seq_length, N)\n",
        "        ).to(device)\n",
        "\n",
        "        trg_positions = (\n",
        "            torch.arange(0, trg_seq_length)\n",
        "            .reshape(trg_seq_length, 1) + torch.zeros(trg_seq_length, N)\n",
        "        ).to(device)\n",
        "\n",
        "        srcWords = self.dropout(self.srcEmbeddings(x.long()) + self.srcPositionalEmbeddings(src_positions.long()))\n",
        "        trgWords = self.dropout(self.trgEmbeddings(trg.long()) + self.trgPositionalEmbeddings(trg_positions.long()))\n",
        "\n",
        "        src_padding_mask = self.make_src_mask(x)\n",
        "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(device)\n",
        "\n",
        "        out = self.transformer(srcWords, trgWords, src_key_padding_mask=src_padding_mask, tgt_mask=trg_mask)\n",
        "        out = self.fc_out(out)\n",
        "        return out"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
