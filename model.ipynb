{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aPmbZt66VWP"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# -model- #\n",
        "\n",
        "def setup_model(model, learning_rate):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    return criterion, optimizer\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.15, max_len=256):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerNet(nn.Module):\n",
        "  def __init__(self, num_vocab, embedding_dim, hidden_size, n_heads, n_layers, max_len, num_labels, dropout):\n",
        "    super(TransformerNet, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(num_vocab, embedding_dim)\n",
        "\n",
        "    self.pe = PositionalEncoding(embedding_dim, max_len = max_len)\n",
        "\n",
        "    enc_layer = nn.TransformerEncoderLayer(embedding_dim, n_heads, hidden_size, dropout)\n",
        "    self.encoder = nn.TransformerEncoder(enc_layer, num_layers = n_layers)\n",
        "\n",
        "    self.dense = nn.Linear(embedding_dim, num_labels)\n",
        "    self.log_softmax = nn.LogSoftmax()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x).permute(1, 0, 2)\n",
        "    x = self.pe(x)\n",
        "    x = self.encoder(x)\n",
        "    x = x.mean(dim=0)\n",
        "    x = self.dense(x)\n",
        "    return x"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}