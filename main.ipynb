{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfktUvggnrDm"
      },
      "source": [
        "import argparse\n",
        "import logging\n",
        "import torch\n",
        "import nltk\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
        "from preprocess import load_multi30k_data, convert_examples_to_features\n",
        "from metric import compute_metrics\n",
        "from model import create_model\n",
        "from train import create_training_args, create_trainer\n",
        "from translate import translate_1\n",
        "nltk.download('punkt')\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Translate German to English using T5 model\")\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"--data_dir\", type=str, default=\"/content/drive/MyDrive/Colab Notebooks/multi30k\",\n",
        "                        help=\"Directory containing data files\")\n",
        "    parser.add_argument(\"--model_ckpt\", type=str, default=\"t5-small\", help=\"Path to the model checkpoint\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"chkpt\", help=\"Directory to save model checkpoints\")\n",
        "    parser.add_argument(\"--max_token_length\", type=int, default=128, help=\"Maximum token length\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Load and preprocess data\n",
        "    dataset = load_multi30k_data(args.data_dir)\n",
        "    tokenizer = T5Tokenizer.from_pretrained(args.model_ckpt)\n",
        "\n",
        "    tokenized_datasets = dataset.map(\n",
        "        lambda examples: convert_examples_to_features(examples, tokenizer, args.max_token_length),\n",
        "        batched=True,\n",
        "        remove_columns=dataset[\"train\"].column_names\n",
        "    )\n",
        "\n",
        "    # Create and train the model\n",
        "    model = create_model(args.model_ckpt, device)\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "    training_args = create_training_args(args.output_dir)\n",
        "    trainer = create_trainer(model, training_args, tokenized_datasets, data_collator, tokenizer, compute_metrics=compute_metrics)\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Translate and evaluate on test data\n",
        "    test_dataloader = DataLoader(\n",
        "        tokenized_datasets[\"test\"], batch_size=32, collate_fn=data_collator\n",
        "    )\n",
        "\n",
        "    eval_preds = trainer.predict(tokenized_datasets[\"valid\"])\n",
        "    metric_result = compute_metrics(eval_preds)\n",
        "    tokenizer = T5Tokenizer.from_pretrained(args.model_ckpt)\n",
        "    logger.info(f'BLEU Score: {metric_result[\"bleu\"]:.4f}')\n",
        "\n",
        "    average_bleu_score = translate_1(model, test_dataloader, tokenizer, args.max_token_length)\n",
        "    logger.info(f'Average BLEU-4 Score: {average_bleu_score:.4f}')\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}