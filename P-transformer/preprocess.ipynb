{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1-HUtr2ArFZ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# -Preprocess- #\n",
        "train_path_en = \"/content/drive/MyDrive/Colab Notebooks/multi30k/train/train.lc.norm.tok.en.txt\"\n",
        "train_path_de = \"/content/drive/MyDrive/Colab Notebooks/multi30k/train/train.lc.norm.tok.de.txt\"\n",
        "test_path_en = \"/content/drive/MyDrive/Colab Notebooks/multi30k/test/test_2017_flickr.lc.norm.tok.en.txt\"\n",
        "test_path_de = \"/content/drive/MyDrive/Colab Notebooks/multi30k/test/test_2017_flickr.lc.norm.tok.de.txt\"\n",
        "\n",
        "with open(train_path_en) as en_raw_train:\n",
        "    en_parsed_train = en_raw_train.readlines()\n",
        "with open(train_path_de) as de_raw_train:\n",
        "    de_parsed_train = de_raw_train.readlines()\n",
        "with open(test_path_en) as en_raw_test:\n",
        "    en_parsed_test = en_raw_test.readlines()\n",
        "with open(test_path_de) as de_raw_test:\n",
        "    de_parsed_test = de_raw_test.readlines()\n",
        "\n",
        "en_train = [sent.strip().split(\" \") for sent in en_parsed_train]\n",
        "en_test = [sent.strip().split(\" \") for sent in en_parsed_test]\n",
        "de_train = [sent.strip().split(\" \") for sent in de_parsed_train]\n",
        "de_test = [sent.strip().split(\" \") for sent in de_parsed_test]\n",
        "\n",
        "en_index2word = [\"<UNK>\", \"<PAD>\", \"<SOS>\", \"<EOS>\"]\n",
        "de_index2word = [\"<UNK>\", \"<PAD>\", \"<SOS>\", \"<EOS>\"]\n",
        "\n",
        "for ds in [en_train, en_test]:\n",
        "    for sent in ds:\n",
        "        for token in sent:\n",
        "            if token not in en_index2word:\n",
        "                en_index2word.append(token)\n",
        "\n",
        "for ds in [de_train, de_test]:\n",
        "    for sent in ds:\n",
        "        for token in sent:\n",
        "            if token not in de_index2word:\n",
        "                de_index2word.append(token)\n",
        "en_word2index = {token: idx for idx, token in enumerate(en_index2word)}\n",
        "de_word2index = {token: idx for idx, token in enumerate(de_index2word)}\n",
        "\n",
        "seq_length = 20\n",
        "def build_vocab(vocab, sent, max_length):\n",
        "    sos = [vocab[\"<SOS>\"]]\n",
        "    eos = [vocab[\"<EOS>\"]]\n",
        "    pad = [vocab[\"<PAD>\"]]\n",
        "\n",
        "    if len(sent) < max_length - 2: # -2 for SOS and EOS\n",
        "        n_pads = max_length - 2 - len(sent)\n",
        "        encoded = [vocab[w] for w in sent]\n",
        "        return sos + encoded + eos + pad * n_pads\n",
        "    else: # sent is longer than max_length; truncating\n",
        "        encoded = [vocab[w] for w in sent]\n",
        "        truncated = encoded[:max_length - 2]\n",
        "        return sos + truncated + eos + pad * (max_length - 2 - len(sent))\n",
        "\n",
        "en_train_dataset = [build_vocab(en_word2index, sent, seq_length) for sent in en_train]\n",
        "en_test_dataset = [build_vocab(en_word2index, sent, seq_length) for sent in en_test]\n",
        "de_train_dataset = [build_vocab(de_word2index, sent, seq_length) for sent in de_train]\n",
        "de_test_dataset = [build_vocab(de_word2index, sent, seq_length) for sent in de_test]\n",
        "\n",
        "def load_data(batch_size, de_train_dataset, en_train_dataset, de_test_dataset, en_test_dataset):\n",
        "    train_x = np.array(de_train_dataset)\n",
        "    train_y = np.array(en_train_dataset)\n",
        "    test_x = np.array(de_test_dataset)\n",
        "    test_y = np.array(en_test_dataset)\n",
        "\n",
        "    train_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "    test_ds = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "    train_loader = DataLoader(train_ds, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "    test_loader = DataLoader(test_ds, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def load_and_preprocess_data(en_path, de_path):\n",
        "\n",
        "    spacy_en = spacy.load('en_core_web_sm')\n",
        "    spacy_de = spacy.load('de_core_news_sm')\n",
        "\n",
        "    with open(test_path_en ) as en_raw_test:\n",
        "        en_parsed_test = en_raw_test.readlines()\n",
        "    with open(test_path_de) as de_raw_test:\n",
        "        de_parsed_test = de_raw_test.readlines()\n",
        "\n",
        "    en_tokenized = [tokenize_en(line, spacy_en) for line in en_parsed_test]\n",
        "    de_tokenized = [tokenize_de(line, spacy_de) for line in de_parsed_test]\n",
        "\n",
        "    test_df = pd.DataFrame({'en': en_tokenized, 'de': de_tokenized})\n",
        "\n",
        "    return test_df\n",
        "\n",
        "def tokenize_de(text, spacy_model):\n",
        "    return [token.text for token in spacy_model(text)]\n",
        "\n",
        "def tokenize_en(text, spacy_model):\n",
        "    return [token.text for token in spacy_model(text)]\n",
        "\n",
        "test = load_and_preprocess_data(test_path_en, test_path_de)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
